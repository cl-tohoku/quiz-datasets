# Quiz Datasets for NLP

This repository maintains question answering (QA) datasets created from Japanese quiz (trivia) questions.


## Downloads

The datasets can be downloaded from the [Releases](https://github.com/cl-tohoku/quiz-datasets/releases) page.


## Dataset formats

The QA datasets generated by `make_dataset.py` are in JSON Lines format.
Specifically, each line in the dataset is a JSON object like the one below:

```js
{
    "qid": "QA20CAPR-0010",
    "competition": "第1回AI王",
    "timestamp": "2019/12/25",
    "section": "開発データ問題 (dev1)",
    "number": "10",
    "original_question": "「鍋についたおこげ」という意味の言葉が語源であるとされる、日本ではマカロニを使ったものが一般的な西洋料理は何でしょう？",
    "original_answer": "グラタン",
    "original_additional_info": "",
    "question": "「鍋についたおこげ」という意味の言葉が語源であるとされる、日本ではマカロニを使ったものが一般的な西洋料理は何でしょう?",
    "answers": ["グラタン"],
    "passages": [
        {
            "passage_id": 269249,
            "title": "グラタン",
            "text": "グラタン(仏: gratin)は、フランスのドーフィネ地方が発祥の地といわれる郷土料理から発達した料理である。「オーブンなどで料理の表面を多少焦がすように調理する」という調理法、およびその調理法を用いて作られた料理の両方を意味する。この調理法を用いたものはすべてグラタンであり、デザート用に作られるものなどもある。主にマカロニがベースとして入ることが多く、後述のドリアとは一線を画している。 日本では、ベシャメルソースを用いオーブンで焼いた料理をして「グラタン」と呼んでいるが、フランス語では、本来鍋に張り付いたおこげという意味でもあり、転じて素材が何であれ焼いて焦げ目をつけた料理を意味する言葉である。"
        },
        {
            "passage_id": 2225997,
            "title": "お焦げ",
            "text": "中華料理、特に四川料理には鍋巴(グオパー、あるいは「中華おこげ」)(en)という料理がある。本来は鍋から掻き取ったお焦げをそのまま使っていた料理で、現代では米飯を乾燥させたものを使用している。揚げた鍋巴にニンジン、白菜、ピーマンなどの野菜や、海老、豚肉などの入ったあんをかけ、溶けて柔らかくなる前のサクサクとした歯ごたえと香ばしさを味わいつつ賞味する。とりわけあんをかける瞬間がこの料理の醍醐味であり、派手な音と立ち上る香りをアピールする為、料理店では客の目の前の卓上でパフォーマンスとして見せるのが定番である。鍋巴を乾燥させる方法は食材として市販されているものは天日で長時間乾燥させたものだが、家庭において米飯を薄く平らに広げたものをフライパンや電子レンジなどで乾燥させることでも代用できる。"
        },
        ...
    ],
    "positive_passage_indices": [0, 18],
    "negative_passage_indices": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, ..., 97, 98, 99]
}
```

Each question is given a maximum of 100 Wikipedia passages retrieved by a full-text search engine.

The `positive_passage_indices` represents the zero-based indices of the passages which contain any of the answers as a substring in the passage's text.
In the above example, the 0th and 18th passages contain the answer "グラタン" as a substring in their texts.
Note that a positive passage does not necessarily contain enough information to reason the answer given the quiestion; they are just classified as positive based on a simple string matching.

Samely, the `negative_passage_indices` represents the zero-based indices of the passages which do not contain any of the answers as a substring in the passage's text.

The datasets are also available in the format of the datasets used in [DPR](https://github.com/facebookresearch/DPR/) so that they are compatible with existing reading comprehension models.
Also, the DPR-formatted datasets are used in [the offical DPR-based baseline system](https://github.com/cl-tohoku/AIO2_DPR_baseline) for the [AIO2 competition](https://sites.google.com/view/project-aio/competition2).


## Steps to generate datasets for AIO2 competition

### Generate QA datasets without passages

The resulting datasets are equivalent to the ones distributed on [the offical website](https://sites.google.com/view/project-aio/home).

```sh
$ mkdir ~/work/quiz-datasets/datasets/no_passages

$ python make_dataset.py \
--input_files data/abc/abc_01.txt data/abc/abc_02.txt data/abc/abc_03.txt data/abc/abc_04.txt data/abc/abc_05.txt data/abc/abc_06.txt data/abc/abc_07.txt data/abc/abc_08.txt data/abc/abc_09.txt data/abc/abc_10.txt data/abc/abc_11.txt data/abc/abc_12.txt \
--output_file ~/work/quiz-datasets/datasets/no_passages/abc_01-12.jsonl.gz
# Total output questions: 17735

$ python make_dataset.py \
--input_files data/aio_jaqket/aio_01_dev1.txt data/aio_jaqket/aio_01_dev2.txt \
--output_file ~/work/quiz-datasets/datasets/no_passages/aio_01_dev.jsonl.gz
# Total output questions: 1992

$ python make_dataset.py \
--input_files data/aio_jaqket/aio_01_test_lb.txt data/aio_jaqket/aio_01_test_lc.txt \
--output_file ~/work/quiz-datasets/datasets/no_passages/aio_01_test.jsonl.gz
# Total output questions: 2000

$ python make_dataset.py \
--input_files data/aio_jaqket/aio_01_unused.txt \
--output_file ~/work/quiz-datasets/datasets/no_passages/aio_01_unused.jsonl.gz
# Total output questions: 608
```

### Generate QA datasets with passages for reading comprehension

#### Requirements

Before executing the scripts, you need to build an Elasticsearch index of Wikipedia passages by running the codes in [singletongue/wikipedia-utils](https://github.com/singletongue/wikipedia-utils).

**Note:** since [Elasticsearch does not always ensure consistent search results](https://www.elastic.co/guide/en/elasticsearch/reference/6.8/consistent-scoring.html), the generated datasets may slightly differ from the distribued ones if you build your own Elasticsearch indices.

#### `jawiki-20211129-sentences-c400-large`

```sh
$ mkdir ~/work/quiz-datasets/datasets/jawiki-20211129-c400-large

$ python make_dataset.py \
--input_files data/abc/abc_01.txt data/abc/abc_02.txt data/abc/abc_03.txt data/abc/abc_04.txt data/abc/abc_05.txt data/abc/abc_06.txt data/abc/abc_07.txt data/abc/abc_08.txt data/abc/abc_09.txt data/abc/abc_10.txt data/abc/abc_11.txt data/abc/abc_12.txt \
--output_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-large/abc_01-12.jsonl.gz \
--num_passages_per_question 100 \
--es_index_name jawiki-20211129-c400 \
--min_inlinks 10 \
--exclude_sexual_pages
# Questions with at least one positive document: 15277
# Questions with no positive document: 2458
# Total output questions: 17735

$ python make_dataset.py \
--input_files data/aio_jaqket/aio_01_dev1.txt data/aio_jaqket/aio_01_dev2.txt \
--output_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-large/aio_01_dev.jsonl.gz \
--num_passages_per_question 100 \
--es_index_name jawiki-20211129-c400 \
--min_inlinks 10 \
--exclude_sexual_pages
# Questions with at least one positive document: 1882
# Questions with no positive document: 110
# Total output questions: 1992

$ python make_dataset.py \
--input_files data/aio_jaqket/aio_01_test_lb.txt data/aio_jaqket/aio_01_test_lc.txt \
--output_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-large/aio_01_test.jsonl.gz \
--num_passages_per_question 100 \
--es_index_name jawiki-20211129-c400 \
--min_inlinks 10 \
--exclude_sexual_pages
# Questions with at least one positive document: 1893
# Questions with no positive document: 107
# Total output questions: 2000

$ python make_dataset.py \
--input_files data/aio_jaqket/aio_01_unused.txt \
--output_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-large/aio_01_unused.jsonl.gz \
--num_passages_per_question 100 \
--es_index_name jawiki-20211129-c400 \
--min_inlinks 10 \
--exclude_sexual_pages
# Questions with at least one positive document: 544
# Questions with no positive document: 64
# Total output questions: 608
```

#### `jawiki-20211129-sentences-c400-medium`

```sh
$ mkdir ~/work/quiz-datasets/datasets/jawiki-20211129-c400-medium

$ python make_dataset.py \
--input_files data/abc/abc_01.txt data/abc/abc_02.txt data/abc/abc_03.txt data/abc/abc_04.txt data/abc/abc_05.txt data/abc/abc_06.txt data/abc/abc_07.txt data/abc/abc_08.txt data/abc/abc_09.txt data/abc/abc_10.txt data/abc/abc_11.txt data/abc/abc_12.txt \
--output_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-medium/abc_01-12.jsonl.gz \
--num_passages_per_question 100 \
--es_index_name jawiki-20211129-c400 \
--min_inlinks 100 \
--exclude_sexual_pages
# Questions with at least one positive passage: 14469
# Questions with no positive passage: 3266
# Total output questions: 17735

$ python make_dataset.py \
--input_files data/aio_jaqket/aio_01_dev1.txt data/aio_jaqket/aio_01_dev2.txt \
--output_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-medium/aio_01_dev.jsonl.gz \
--num_passages_per_question 100 \
--es_index_name jawiki-20211129-c400 \
--min_inlinks 100 \
--exclude_sexual_pages
# Questions with at least one positive passage: 1777
# Questions with no positive passage: 215
# Total output questions: 1992

$ python make_dataset.py \
--input_files data/aio_jaqket/aio_01_test_lb.txt data/aio_jaqket/aio_01_test_lc.txt \
--output_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-medium/aio_01_test.jsonl.gz \
--num_passages_per_question 100 \
--es_index_name jawiki-20211129-c400 \
--min_inlinks 100 \
--exclude_sexual_pages
# Questions with at least one positive passage: 1761
# Questions with no positive passage: 239
# Total output questions: 2000

$ python make_dataset.py \
--input_files data/aio_jaqket/aio_01_unused.txt \
--output_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-medium/aio_01_unused.jsonl.gz \
--num_passages_per_question 100 \
--es_index_name jawiki-20211129-c400 \
--min_inlinks 100 \
--exclude_sexual_pages
# Questions with at least one positive passage: 509
# Questions with no positive passage: 99
# Total output questions: 608
```

#### `jawiki-20211129-sentences-c400-small`

```sh
$ mkdir ~/work/quiz-datasets/datasets/jawiki-20211129-c400-small

$ python make_dataset.py \
--input_files data/abc/abc_01.txt data/abc/abc_02.txt data/abc/abc_03.txt data/abc/abc_04.txt data/abc/abc_05.txt data/abc/abc_06.txt data/abc/abc_07.txt data/abc/abc_08.txt data/abc/abc_09.txt data/abc/abc_10.txt data/abc/abc_11.txt data/abc/abc_12.txt \
--output_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-small/abc_01-12.jsonl.gz \
--num_passages_per_question 100 \
--es_index_name jawiki-20211129-c400 \
--min_inlinks 500 \
--exclude_sexual_pages
# Questions with at least one positive passage: 12368
# Questions with no positive passage: 5367
# Total output questions: 17735

$ python make_dataset.py \
--input_files data/aio_jaqket/aio_01_dev1.txt data/aio_jaqket/aio_01_dev2.txt \
--output_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-small/aio_01_dev.jsonl.gz \
--num_passages_per_question 100 \
--es_index_name jawiki-20211129-c400 \
--min_inlinks 500 \
--exclude_sexual_pages
# Questions with at least one positive passage: 1509
# Questions with no positive passage: 483
# Total output questions: 1992

$ python make_dataset.py \
--input_files data/aio_jaqket/aio_01_test_lb.txt data/aio_jaqket/aio_01_test_lc.txt \
--output_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-small/aio_01_test.jsonl.gz \
--num_passages_per_question 100 \
--es_index_name jawiki-20211129-c400 \
--min_inlinks 500 \
--exclude_sexual_pages
# Questions with at least one positive passage: 1483
# Questions with no positive passage: 517
# Total output questions: 2000

$ python make_dataset.py \
--input_files data/aio_jaqket/aio_01_unused.txt \
--output_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-small/aio_01_unused.jsonl.gz \
--num_passages_per_question 100 \
--es_index_name jawiki-20211129-c400 \
--min_inlinks 500 \
--exclude_sexual_pages
# Questions with at least one positive passage: 430
# Questions with no positive passage: 178
# Total output questions: 608
```

### Convert the datasets into the format of DPR Retriever input files

```sh
# jawiki-20211129-sentences-c400-large
$ mkdir ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-large
$ python convert_dataset_to_dpr_retriever_input_file.py \
--input_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-large/abc_01-12.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-large/abc_01-12.json.gz
$ python convert_dataset_to_dpr_retriever_input_file.py \
--input_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-large/aio_01_dev.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-large/aio_01_dev.json.gz
$ python convert_dataset_to_dpr_retriever_input_file.py \
--input_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-large/aio_01_test.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-large/aio_01_test.json.gz
$ python convert_dataset_to_dpr_retriever_input_file.py \
--input_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-large/aio_01_unused.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-large/aio_01_unused.json.gz

# jawiki-20211129-sentences-c400-medium
$ mkdir ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-medium
$ python convert_dataset_to_dpr_retriever_input_file.py \
--input_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-medium/abc_01-12.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-medium/abc_01-12.json.gz
$ python convert_dataset_to_dpr_retriever_input_file.py \
--input_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-medium/aio_01_dev.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-medium/aio_01_dev.json.gz
$ python convert_dataset_to_dpr_retriever_input_file.py \
--input_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-medium/aio_01_test.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-medium/aio_01_test.json.gz
$ python convert_dataset_to_dpr_retriever_input_file.py \
--input_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-medium/aio_01_unused.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-medium/aio_01_unused.json.gz

# jawiki-20211129-sentences-c400-small
$ mkdir ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-small
$ python convert_dataset_to_dpr_retriever_input_file.py \
--input_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-small/abc_01-12.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-small/abc_01-12.jsonl.gz
$ python convert_dataset_to_dpr_retriever_input_file.py \
--input_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-small/aio_01_dev.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-small/aio_01_dev.jsonl.gz
$ python convert_dataset_to_dpr_retriever_input_file.py \
--input_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-small/aio_01_test.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-small/aio_01_test.jsonl.gz
$ python convert_dataset_to_dpr_retriever_input_file.py \
--input_file ~/work/quiz-datasets/datasets/jawiki-20211129-c400-small/aio_01_unused.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/retriever/jawiki-20211129-c400-small/aio_01_unused.jsonl.gz
```

### Convert the datasets into the format of DPR questions CSV files

```sh
$ python convert_dataset_to_dpr_qas_file.py \
--input_file ~/work/quiz-datasets/datasets/no_passages/abc_01-12.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/qas/abc_01-12.tsv
$ python convert_dataset_to_dpr_qas_file.py \
--input_file ~/work/quiz-datasets/datasets/no_passages/aio_01_dev.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/qas/aio_01_dev.tsv
$ python convert_dataset_to_dpr_qas_file.py \
--input_file ~/work/quiz-datasets/datasets/no_passages/aio_01_test.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/qas/aio_01_test.tsv
$ python convert_dataset_to_dpr_qas_file.py \
--input_file ~/work/quiz-datasets/datasets/no_passages/aio_01_unused.jsonl.gz \
--output_file ~/work/quiz-datasets/dpr/qas/aio_01_unused.tsv
```

### Convert the passages file into DPR passages TSV files

```sh
# Filtered pages large (4,188,183 passages)
$ python convert_passages_to_dpr_format.py \
--passages_file ~/work/wikipedia-utils/20211129/passages-c400-jawiki-20211129.json.gz \
--page_ids_file ~/work/wikipedia-utils/20211129/page-ids-jawiki-20211129.json \
--output_file ~/work/quiz-datasets/dpr/wikipedia_split/jawiki-20211129-c400-large.tsv.gz \
--min_inlinks 10 \
--exclude_sexual_pages

# Filtered pages medium (1,627,879 passages)
$ python convert_passages_to_dpr_format.py \
--passages_file ~/work/wikipedia-utils/20211129/passages-c400-jawiki-20211129.json.gz \
--page_ids_file ~/work/wikipedia-utils/20211129/page-ids-jawiki-20211129.json \
--output_file ~/work/quiz-datasets/dpr/wikipedia_split/jawiki-20211129-c400-medium.tsv.gz \
--min_inlinks 100 \
--exclude_sexual_pages

# Filtered pages small (379,096 passages)
$ python convert_passages_to_dpr_format.py \
--passages_file ~/work/wikipedia-utils/20211129/passages-c400-jawiki-20211129.json.gz \
--page_ids_file ~/work/wikipedia-utils/20211129/page-ids-jawiki-20211129.json \
--output_file ~/work/quiz-datasets/dpr/wikipedia_split/jawiki-20211129-c400-small.tsv.gz \
--min_inlinks 500 \
--exclude_sexual_pages
```


## License

- The codes in this repository, not including the datasets themselves, are distributed under the MIT license.

- `data/abc` 以下に含まれるクイズ問題の著作権は [abc/EQIDEN 実行委員会](http://abc-dive.com/questions/) に帰属します。東北大学において研究目的での再配布許諾を得ています。
- `data/aio_jaqket` 以下に含まれるクイズ問題は [Creative Commons Attribution-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/) ライセンスの下に提供されています。これらのクイズ問題は [株式会社キュービック](http://www.qbik.co.jp/) および [クイズ法人カプリティオ](http://capriccio.tokyo/) に依頼し作成したものです。
- 読解データセットにパッセージとして付与されている Wikipedia のコンテンツは、[Attribution-ShareAlike 3.0 Unported](https://creativecommons.org/licenses/by-sa/3.0/) ライセンスおよび [GFDL](https://www.gnu.org/copyleft/fdl.html) ライセンスの下に配布されているものです。
